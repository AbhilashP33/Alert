# c86_alerts_cards.py
# Port of c86_Alerts_Cards.sas to Python + Trino/Starburst
# Option A: Line-by-line replica of SAS logic
# Python 3.10+, pandas 2.x

import os
import sys
import math
import logging
import numpy as np
from pathlib import Path
from datetime import datetime, date, timedelta

import pandas as pd
import trino
from trino.auth import BasicAuthentication

# =============================================================================
# CONFIG & LOGGING
# =============================================================================

def setup_logging(log_path: Path):
    log_path.parent.mkdir(parents=True, exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] - %(message)s",
        handlers=[logging.FileHandler(log_path), logging.StreamHandler(sys.stdout)],
    )

def load_config(config_path: Path = Path("config.ini")) -> dict:
    import configparser
    cfg = configparser.ConfigParser()
    if not config_path.exists():
        raise FileNotFoundError(f"Missing config file: {config_path}")
    cfg.read(config_path)
    c = cfg["trino"]
    return {
        "host": c.get("host"),
        "port": c.getint("port", fallback=443),
        "user": c.get("user"),
        "password": c.get("password", fallback=None),
        "http_scheme": c.get("http_scheme", fallback="https"),
        "catalog": c.get("catalog", fallback="hive"),
        "schema": c.get("schema", fallback="default"),
        "source": c.get("source", fallback="c86_alerts_cards"),
        "regpath": c.get("regpath", fallback="/sas/RSD/REG"),
        "out_root": c.get("out_root", fallback="./c86/output/alert/cards"),
        "log_root": c.get("log_root", fallback="./c86/log/alert/cards"),
        "env": c.get("env", fallback="PROD"),
    }

def get_trino_conn(conf: dict):
    auth = None
    if conf.get("password"):
        auth = BasicAuthentication(conf["user"], conf["password"])
    return trino.dbapi.connect(
        host=conf["host"],
        port=conf["port"],
        user=conf["user"],
        http_scheme=conf["http_scheme"],
        catalog=conf["catalog"],
        schema=conf["schema"],
        source=conf["source"],
        auth=auth,
    )

# =============================================================================
# SAS DATE LOGIC
# =============================================================================

def compute_dates(today: date | None = None) -> dict:
    if not today:
        today = date.today()

    start_dt_ini = date(2022, 6, 30)
    week_end_dt_ini = date(2022, 7, 25)

    weekday = today.weekday()
    days_since_friday = (weekday - 4) % 7
    week_start_friday = today - timedelta(days=days_since_friday)
    end_dt = week_start_friday - timedelta(days=2)

    if end_dt < week_end_dt_ini:
        start_dt = start_dt_ini
    else:
        start_dt = end_dt - timedelta(days=6)

    week_start_dt = start_dt
    week_end_dt = end_dt
    pardt = start_dt - timedelta(days=7)
    week_end_dt_p1 = end_dt + timedelta(days=1)

    label = today.strftime("%Y%m%d")
    return {
        "report_dt": today,
        "start_dt": start_dt,
        "end_dt": end_dt,
        "week_start_dt": week_start_dt,
        "week_end_dt": week_end_dt,
        "week_end_dt_p1": week_end_dt_p1,
        "pardt": pardt,
        "label": label,
        "ymd": label,
    }

# =============================================================================
# TRINO SQL HELPERS (Corrected)
# =============================================================================

ESS_PROCESS_TS = """
    try(CAST(from_iso8601_timestamp(element_at(eventAttributes, 'ess_process_timestamp')) AS timestamp))
"""
ESS_SRC_TS = """
    try(CAST(from_iso8601_timestamp(element_at(eventAttributes, 'ess_src_event_timestamp')) AS timestamp))
"""
SRC_HDR = "try(json_parse(element_at(eventAttributes, 'SourceEventHeader')))"
EVT_PAYLOAD = "try(json_parse(element_at(eventAttributes, 'eventPayload')))"
EVT_ID = f"try(json_extract_scalar({SRC_HDR}, '$.eventId'))"
EVT_TS = f"""
    try(CAST(from_iso8601_timestamp(json_extract_scalar({SRC_HDR}, '$.eventTimestamp')) AS timestamp))
"""

def q_xb80_cards(schema: str, week_start: date, week_end_p1: date, pardt: date):
    return f"""
    SELECT
        event_activity_type,
        source_event_id,
        partition_date,
        {ESS_PROCESS_TS} AS ess_process_timestamp,
        {ESS_SRC_TS}     AS ess_src_event_timestamp,
        {EVT_ID}         AS eventId,
        {EVT_TS}         AS eventTimestamp,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.accountId')) AS accountId,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.alertType')) AS alertType,
        try(CAST(json_extract_scalar({EVT_PAYLOAD}, '$.thresholdAmount') AS DECIMAL(10,2))) AS thresholdAmount,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.customerID')) AS customerID,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.accountCurrency')) AS accountCurrency,
        try(CAST(json_extract_scalar({EVT_PAYLOAD}, '$.creditLimit') AS DECIMAL(10,2))) AS creditLimit,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.maskedAccount')) AS maskedAccount,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.decisionId')) AS decisionId,
        try(CAST(json_extract_scalar({EVT_PAYLOAD}, '$.alertAmount') AS DECIMAL(10,2))) AS alertAmount
    FROM {schema}.xb80_credit_card_system_interface
    WHERE
        partition_date > DATE '{pardt}'
        AND event_activity_type = 'Alert Decision Cards'
        AND try(json_extract_scalar({EVT_PAYLOAD}, '$.alertType')) = 'AVAIL_CREDIT_REMAINING'
        AND event_timestamp BETWEEN TIMESTAMP '{week_start} 00:00:00'
                              AND TIMESTAMP '{week_end_p1} 00:00:00'
    """

def q_cards_dec_pref(schema: str, week_end_p1: date):
    a = f"""
        SELECT
            {EVT_TS} AS event_timestamp_p,
            event_channel_type AS event_channel_type_p,
            event_activity_type AS event_activity_type_p,
            partition_date AS partition_date_p,
            {ESS_PROCESS_TS} AS ess_process_timestamp_p,
            {ESS_SRC_TS} AS ess_src_event_timestamp_p,
            {EVT_TS} AS eventTimestamp_p,
            try(json_extract_scalar({SRC_HDR}, '$.eventActivityName')) AS eventActivityName_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.preferenceType')) AS preferenceType_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.clientID')) AS clientID_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.isBusiness')) AS isBusiness_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.sendAlertEligible')) AS sendAlertEligible_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.active')) AS active_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.threshold')) AS threshold_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.custID')) AS custID_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.account')) AS account_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.maskAccountNo')) AS maskedAccountNo_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.externalAccount')) AS externalAccount_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.productType')) AS productType_p
        FROM {schema}.ffs0_client_alert_preferences_dep_initial_load
        WHERE event_activity_type IN ('Create Account Preference','Update Account Preference')
          AND try(json_extract_scalar({EVT_PAYLOAD}, '$.preferenceType')) = 'AVAIL_CREDIT_REMAINING'
          AND try(json_extract_scalar({EVT_PAYLOAD}, '$.productType')) = 'CREDIT_CARD'
          AND partition_date = DATE '2022-03-24'
    """
    b = f"""
        SELECT
            {EVT_TS} AS event_timestamp_p,
            event_channel_type AS event_channel_type_p,
            event_activity_type AS event_activity_type_p,
            partition_date AS partition_date_p,
            {ESS_PROCESS_TS} AS ess_process_timestamp_p,
            {ESS_SRC_TS} AS ess_src_event_timestamp_p,
            {EVT_TS} AS eventTimestamp_p,
            try(json_extract_scalar({SRC_HDR}, '$.eventActivityName')) AS eventActivityName_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.preferenceType')) AS preferenceType_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.clientID')) AS clientID_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.isBusiness')) AS isBusiness_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.sendAlertEligible')) AS sendAlertEligible_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.active')) AS active_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.threshold')) AS threshold_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.custID')) AS custID_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.account')) AS account_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.maskAccountNo')) AS maskedAccountNo_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.externalAccount')) AS externalAccount_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.productType')) AS productType_p
        FROM {schema}.ffs0_client_alert_preferences_dep
        WHERE event_timestamp > TIMESTAMP '{week_end_p1} 00:00:00'
          AND event_activity_type IN ('Create Account Preference','Update Account Preference')
          AND try(json_extract_scalar({EVT_PAYLOAD}, '$.preferenceType')) = 'AVAIL_CREDIT_REMAINING'
          AND try(json_extract_scalar({EVT_PAYLOAD}, '$.productType')) = 'CREDIT_CARD'
    """

    return f"SELECT * FROM ({a}) UNION SELECT * FROM ({b})"

def q_fft0_inbox(schema: str, week_start: date):
    return f"""
    SELECT
        {ESS_PROCESS_TS} AS ess_process_timestamp_a,
        event_activity_type AS event_activity_type_a,
        source_event_id AS source_event_id_a,
        partition_date AS partition_date_a,
        event_timestamp AS event_timestamp_a,
        {EVT_ID} AS eventId_a,
        {EVT_TS} AS eventTimestamp_a,
        try(json_extract_scalar({SRC_HDR}, '$.eventActivityName')) AS eventActivityName_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.alertSent')) AS alertSent_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.sendInbox')) AS sendInbox_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.alertType')) AS alertType_a,
        try(CAST(json_extract_scalar({EVT_PAYLOAD}, '$.thresholdAmount') AS DECIMAL(12,2))) AS thresholdAmount_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.sendSMS')) AS sendSMS_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.sendPush')) AS sendPush_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.maskedAccount')) AS maskedAccount_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.reasonCode')) AS reasonCode_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.decisionId')) AS decisionId_a,
        try(CAST(json_extract_scalar({EVT_PAYLOAD}, '$.alertAmount') AS DECIMAL(12,2))) AS alertAmount_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.accountID')) AS accountId_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.account')) AS account_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.accountProduct')) AS accountProduct_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.sendEmail')) AS sendEmail_a
    FROM {schema}.fft0_alert_inbox_dep
    WHERE event_activity_type = 'Alert Delivery Audit'
      AND try(json_extract_scalar({EVT_PAYLOAD}, '$.alertType')) = 'AVAIL_CREDIT_REMAINING'
      AND event_timestamp > TIMESTAMP '{week_start} 00:00:00'
    """

# =============================================================================
# READ DF
# =============================================================================

def read_trino_df(conn, sql: str) -> pd.DataFrame:
    logging.info("\nRunning SQL:\n%s", sql)
    return pd.read_sql(sql, conn)

# =============================================================================
# PIPELINE LOGIC
# =============================================================================

def run_pipeline(conf: dict):

    dt = compute_dates()
    label = dt["label"]

    out_root = Path(conf["out_root"])
    log_root = Path(conf["log_root"])
    out_dir = out_root / label
    out_dir.mkdir(parents=True, exist_ok=True)
    log_root.mkdir(parents=True, exist_ok=True)

    log_file = log_root / f"c8600J_Alerts_Cards_{label}.log"
    setup_logging(log_file)

    logging.info("Environment=%s Catalog=%s Schema=%s", conf["env"], conf["catalog"], conf["schema"])
    logging.info("Output dir: %s", out_dir.resolve())

    with get_trino_conn(conf) as conn:
        schema = f"{conf['catalog']}.{conf['schema']}"
        df_cards = read_trino_df(conn, q_xb80_cards(schema, dt["week_start_dt"], dt["week_end_dt_p1"], dt["pardt"]))
        df_pref  = read_trino_df(conn, q_cards_dec_pref(schema, dt["week_end_dt_p1"]))
        df_inbox = read_trino_df(conn, q_fft0_inbox(schema, dt["week_start_dt"]))

    # === CLEAN TS ===
    ts_cols_cards = ["eventTimestamp","ess_process_timestamp","ess_src_event_timestamp"]
    ts_cols_pref  = ["event_timestamp_p","eventTimestamp_p","ess_process_timestamp_p","ess_src_event_timestamp_p"]

    for c in ts_cols_cards:
        if c in df_cards.columns:
            df_cards[c] = pd.to_datetime(df_cards[c], errors="coerce")
    for c in ts_cols_pref:
        if c in df_pref.columns:
            df_pref[c] = pd.to_datetime(df_pref[c], errors="coerce")

    df_cards = df_cards.rename(columns={"accountid":"accountid_key","customerid":"customerid_key"})
    df_pref = df_pref.rename(columns={"externalaccount_p":"externalaccount_p_key","custid_p":"custid_p_key"})

    # === JOIN PREF ===
    df_join = pd.merge(
        df_cards,
        df_pref,
        how="left",
        left_on=["accountid_key","customerid_key"],
        right_on=["externalaccount_p_key","custid_p_key"],
    )

    df_join["dec_tm_ge_pref_tm"] = (
        (df_join["eventTimestamp"] > df_join["eventTimestamp_p"]).map({True:"Y",False:"N"})
    ).fillna("N")

    df_join = df_join.sort_values(
        by=[
            "decisionid","eventTimestamp","accountid_key","customerid_key",
            "dec_tm_ge_pref_tm","eventTimestamp_p","externalaccount_p_key"
        ],
        ascending=[True,True,True,True,False,False,False]
    )

    df_cards_dec_pref2 = df_join.drop_duplicates(subset=["decisionid"], keep="first")

    # === PREP INBOX ===
    for c in ["eventTimestamp_a","ess_process_timestamp_a"]:
        if c in df_inbox.columns:
            df_inbox[c] = pd.to_datetime(df_inbox[c], errors="coerce")

    df_inbox = df_inbox.sort_values(
        by=["decisionid_a","eventTimestamp_a"],
        ascending=[True,False]
    ).drop_duplicates(subset=["decisionid_a"], keep="first")

    # === JOIN INBOX ===
    df_all = pd.merge(
        df_cards_dec_pref2,
        df_inbox,
        how="left",
        left_on="decisionid",
        right_on="decisionid_a"
    )

    # === FILTER isbusiness_p / dec_tm ===
    def to_bool_str(x): return str(x).lower()

    df_final = df_all[
        ~((df_all["isbusiness_p"].map(to_bool_str)=="true") &
          (df_all["dec_tm_ge_pref_tm"]=="Y"))
    ].copy()

    # === DATES ===
    for c in ["ess_src_event_timestamp","ess_process_timestamp","eventTimestamp","eventTimestamp_a"]:
        if c in df_final.columns:
            df_final[c] = pd.to_datetime(df_final[c], errors="coerce")

    df_final["src_event_date"] = df_final["ess_src_event_timestamp"].dt.date
    df_final["process_date"]   = df_final["ess_process_timestamp"].dt.date
    df_final["decision_date"]  = df_final["eventTimestamp"].dt.date
    df_final["event_date"]     = df_final["eventTimestamp"].dt.date

    df_final["alertamount"] = pd.to_numeric(df_final["alertamount"], errors="coerce")
    df_final["thresholdamount"] = pd.to_numeric(df_final["thresholdamount"], errors="coerce")
    df_final["threshold_limit_check"] = (
        df_final["alertamount"] < df_final["thresholdamount"]
    ).map({True:"Y",False:"N"})

    df_final["Found_Missing"] = (
        df_final["eventTimestamp_a"].isna() | df_final["eventTimestamp"].isna()
    ).map({True:"Y",False:"N"})

    df_final["Time_Diff"] = (
        df_final["eventTimestamp_a"] - df_final["eventTimestamp"]
    ).dt.total_seconds()

    df_final["SLA_Ind"] = np.where(
        (df_final["Time_Diff"].notna()) & (df_final["Time_Diff"] <= 1800),
        "Y","N"
    )

    bins = [-float("inf"),0,1800,3600,7200,10800,14400,18000,21600,
            25200,28800,32400,36000,86400,172800,259200,float("inf")]

    labels = [
        "00 - Less than 0 seconds",
        "01 - Less than or equal to 30 minutes",
        "02 - Greater than 30 mins and less than or equal to 60 mins",
        "03 - Greater than 1 hour and less than or equal     to 2 hours",
        "04 - Greater than 2 hours and less than or equal    to 3 hours",
        "05 - Greater than 3 hours and less than or equal    to 4 hours",
        "06 - Greater than 4 hours and less than or equal    to 5 hours",
        "07 - Greater than 5 hours and less than or equal    to 6 hours",
        "08 - Greater than 6 hours and less than or equal    to 7 hours",
        "09 - Greater than 7 hours and less than or equal    to 8 hours",
        "10 - Greater than 8 hours and less than or equal    to 9 hours",
        "11 - Greater than 9 hours and less than or equal    to 10 hours",
        "12 - Greater than 10 hours and less than or equal   to 24 hours",
        "13 - Greater than 1 day and less than or equal      to 2 days",
        "14 - Greater than 2 days and less than or equal     to 3 days",
        "15 - Greater than 3 days"
    ]

    df_final["Alert_Time"] = pd.cut(df_final["Time_Diff"], bins=bins, labels=labels)
    df_final["Alert_Time"] = df_final["Alert_Time"].cat.add_categories(["99 - Timestamp is missing"])
    df_final["Alert_Time"] = df_final["Alert_Time"].fillna("99 - Timestamp is missing")

    # =============================================================================
    # SUMMARIES & SAMPLE
    # =============================================================================

    report_dt = dt["report_dt"]

    def snapdate(d):
        if pd.isna(d):
            return pd.NaT
        weekday = d.weekday()
        days_to_wed = (2 - weekday) % 7
        return d + timedelta(days=days_to_wed)

    def sample_by_day(df, size=10, seed=42):
        parts = []
        for k, g in df.groupby("decision_date"):
            if len(g) <= size:
                parts.append(g.copy())
            else:
                parts.append(g.sample(n=size, random_state=seed))
        return pd.concat(parts, ignore_index=True) if parts else df.head(0)

    sample_df = sample_by_day(df_final, size=10)

    # ACCURACY
    acc = (
        sample_df.assign(
            ControlRisk="Accuracy",
            TestType="Sample",
            RDE="Alert010_Accuracy_Available_Credit",
            CommentCode=lambda d: np.where(d["threshold_limit_check"]=="Y","COM16","COM19"),
            Segment10=lambda d: pd.to_datetime(d["decision_date"]).dt.strftime("%Y%m%d"),
            DateCompleted=report_dt,
            SnapDate=lambda d: pd.to_datetime(d["decision_date"]).apply(snapdate),
        )
        .groupby(["ControlRisk","TestType","RDE","CommentCode","Segment10","DateCompleted","SnapDate"], dropna=False)
        .agg(Volume=("decisionid","count"),
             bal=("alertamount","sum"),
             Amount=("thresholdamount","sum"))
        .reset_index()
    )

    # TIMELINESS
    tml = (
        df_final.assign(
            ControlRisk="Timeliness",
            TestType="Anomaly",
            RDE="Alert011_Timeliness_SLA",
            CommentCode=lambda d: np.where(d["SLA_Ind"]=="Y","COM16","COM19"),
            Segment10=lambda d: pd.to_datetime(d["decision_date"]).dt.strftime("%Y%m%d"),
            DateCompleted=report_dt,
            SnapDate=lambda d: pd.to_datetime(d["decision_date"]).apply(snapdate),
        )
        .groupby(["ControlRisk","TestType","RDE","CommentCode","Segment10","DateCompleted","SnapDate"], dropna=False)
        .agg(Volume=("decisionid","count"),
             bal=("alertamount","sum"),
             Amount=("thresholdamount","sum"))
        .reset_index()
    )

    # COMPLETENESS
    cpl = (
        df_final.assign(
            ControlRisk="Completeness",
            TestType="Reconciliation",
            RDE="Alert012_Completeness_All_Clients",
            CommentCode=lambda d: np.where(d["decisionid_a"].fillna("")!="","COM16","COM19"),
            Segment10=lambda d: pd.to_datetime(d["decision_date"]).dt.strftime("%Y%m"),
            DateCompleted=report_dt,
            SnapDate=lambda d: pd.to_datetime(d["decision_date"]).apply(snapdate),
        )
        .groupby(["ControlRisk","TestType","RDE","CommentCode","Segment10","DateCompleted","SnapDate"], dropna=False)
        .agg(Volume=("decisionid","count"),
             bal=("alertamount","sum"),
             Amount=("thresholdamount","sum"))
        .reset_index()
    )

    alert_cards_ac_week = pd.concat([acc,tml,cpl], ignore_index=True)

    # ADD FIXED COLS
    alert_cards_ac_week.insert(0,"RegulatoryName","c86")
    alert_cards_ac_week.insert(1,"LOB","Credit Cards")
    alert_cards_ac_week.insert(2,"ReportName","c86 Alerts")
    alert_cards_ac_week.insert(5,"TestPeriod","Portfolio")
    alert_cards_ac_week.insert(6,"ProductType","Credit Cards")
    for c in ["SubDE","Segment","Segment2","Segment3","Segment4",
              "Segment5","Segment6","Segment7","Segment8","Segment9"]:
        alert_cards_ac_week[c]="."
    alert_cards_ac_week["HoldoutFlag"]="N"
    alert_cards_ac_week["Comments"]=""

    # =============================================================================
    # DETAILS
    # =============================================================================

    completeness_fail = (
        df_final[df_final["decisionid_a"].fillna("")==""]
        .assign(
            event_month=lambda d: pd.to_datetime(d["decision_date"]).dt.strftime("%Y%m"),
            reporting_date=report_dt,
            event_week_ending=lambda d: pd.to_datetime(d["decision_date"]).apply(snapdate),
            LOB="Credit Cards",
            Product="Credit Cards",
            account_number=lambda d: d["accountid_key"],
            available_credit=lambda d: d["alertamount"],
            event_date=lambda d: pd.to_datetime(d["decision_date"]).dt.date,
            custid_mask=lambda d: d["customerid_key"].fillna("").str[-3:].radd("******")
        )[
            ["event_month","reporting_date","event_week_ending","LOB","Product",
             "account_number","thresholdamount","available_credit","decisionid",
             "event_date","custid_mask"]
        ]
    )

    timeliness_fail = (
        df_final[df_final["SLA_Ind"]!="Y"]
        .assign(
            event_month=lambda d: pd.to_datetime(d["decision_date"]).dt.strftime("%Y%m"),
            reporting_date=report_dt,
            event_week_ending=lambda d: pd.to_datetime(d["decision_date"]).apply(snapdate),
            LOB="Credit Cards",
            Product="Credit Cards",
            account_number=lambda d: d["accountid_key"],
            available_credit=lambda d: d["alertamount"],
            event_date=lambda d: pd.to_datetime(d["decision_date"]).dt.date,
            total_minutes=lambda d: (d["Time_Diff"]/60).apply(lambda x: None if pd.isna(x) else math.ceil(x)),
            custid_mask=lambda d: d["customerid_key"].fillna("").str[-3:].radd("******"),
        )[
            ["event_month","reporting_date","event_week_ending","LOB","Product",
             "account_number","thresholdamount","available_credit","decisionid",
             "event_date","eventTimestamp","eventTimestamp_a","total_minutes","custid_mask"]
        ]
    )

    accuracy_fail = (
        sample_df[sample_df["threshold_limit_check"]!="Y"]
        .assign(
            event_month=lambda d: pd.to_datetime(d["decision_date"]).dt.strftime("%Y%m"),
            reporting_date=report_dt,
            event_week_ending=lambda d: pd.to_datetime(d["decision_date"]).apply(snapdate),
            LOB="Credit Cards",
            Product="Credit Cards",
            account_number=lambda d: d["accountid_key"],
            decision="AlertDecision",
            available_credit=lambda d: d["alertamount"],
            event_date=lambda d: pd.to_datetime(d["decision_date"]).dt.date,
            custid_mask=lambda d: d["customerid_key"].fillna("").str[-3:].radd("******"),
        )[
            ["event_month","reporting_date","event_week_ending","LOB","Product",
             "account_number","decision","thresholdamount","available_credit","decisionid",
             "event_date","custid_mask"]
        ]
    )

    # =============================================================================
    # OUTPUTS
    # =============================================================================

    df_final_path = out_dir / "cards_alert_final.parquet"
    df_final.to_parquet(df_final_path, index=False)

    ac_week_path = out_dir / "Alert_Cards_AC_week.parquet"
    alert_cards_ac_week.to_parquet(ac_week_path, index=False)

    completeness_fail.to_excel(out_dir/"Alert_Cards_Completeness_Detail.xlsx",
                               sheet_name="Alert_Cards_Completeness_Detail",
                               index=False)

    timeliness_fail.to_excel(out_dir/"Alert_Cards_Timeliness_Detail.xlsx",
                             sheet_name="Alert_Cards_Timeliness_Detail",
                             index=False)

    accuracy_fail.to_excel(out_dir/"Alert_Cards_Accuracy_Detail.xlsx",
                           sheet_name="Alert_Cards_Accuracy_Detail",
                           index=False)

    logging.info("Final count: %d rows, %d distinct decisionId",
                 len(df_final), df_final["decisionid"].nunique())

    # =============================================================================
    # HISTORICAL APPEND
    # =============================================================================

    ac_lib_path = Path(conf["out_root"]).parent
    ac_lib_path.mkdir(parents=True, exist_ok=True)
    historical_file = ac_lib_path/"alert_cards_ac.parquet"

    df_new = alert_cards_ac_week.copy()
    df_new["SnapDate"] = pd.to_datetime(df_new["SnapDate"]).dt.date

    if historical_file.exists():
        df_hist = pd.read_parquet(historical_file)
        df_hist["SnapDate"] = pd.to_datetime(df_hist["SnapDate"]).dt.date
        new_snaps = df_new["SnapDate"].unique()
        df_hist_f = df_hist[~df_hist["SnapDate"].isin(new_snaps)]
        df_combined = pd.concat([df_hist_f, df_new], ignore_index=True)
    else:
        df_combined = df_new

    df_combined = df_combined.sort_values("SnapDate")
    df_combined.to_parquet(historical_file, index=False)

# =============================================================================
# MAIN
# =============================================================================

if __name__ == "__main__":
    cfg = load_config(Path("config.ini"))
    run_pipeline(cfg)
