# c86_alerts_cards.py
# Port of c86_Alerts_Cards.sas to Python + Trino/Starburst
# Python 3.10+, pandas 2.x

import sys
import math
import logging
from pathlib import Path
from datetime import date, timedelta

import numpy as np
import pandas as pd

import trino
from trino.auth import BasicAuthentication
# from trino.auth import OAuth2Authentication  # if Starburst SSO/OIDC is used


# =============================================================================
# 0. CONFIG / LOGGING
# =============================================================================

def setup_logging(log_path: Path):
    log_path.parent.mkdir(parents=True, exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] - %(message)s",
        handlers=[logging.FileHandler(log_path), logging.StreamHandler(sys.stdout)],
    )


def load_config(config_path: Path = Path("config.ini")) -> dict:
    """
    Minimal INI loader. Expects a section [trino].
    """
    import configparser

    cfg = configparser.ConfigParser()
    if not config_path.exists():
        raise FileNotFoundError(f"Missing config file: {config_path}")
    cfg.read(config_path)

    c = cfg["trino"]
    return {
        "host": c.get("host"),
        "port": c.getint("port", fallback=443),
        "user": c.get("user"),
        "password": c.get("password", fallback=None),
        "http_scheme": c.get("http_scheme", fallback="https"),
        "catalog": c.get("catalog", fallback="hive"),
        "schema": c.get("schema", fallback="default"),
        "source": c.get("source", fallback="c86_alerts_cards"),
        "regpath": c.get("regpath", fallback="/sas/RSD/REG"),
        "out_root": c.get("out_root", fallback="./c86/output/alert/cards"),
        "log_root": c.get("log_root", fallback="./c86/log/alert/cards"),
        "env": c.get("env", fallback="PROD"),
    }


def get_trino_conn(conf: dict):
    auth = None
    if conf.get("password"):
        auth = BasicAuthentication(conf["user"], conf["password"])
    # For Starburst with OAuth2 / SSO, swap auth above accordingly.

    return trino.dbapi.connect(
        host=conf["host"],
        port=conf["port"],
        user=conf["user"],
        http_scheme=conf["http_scheme"],
        auth=auth,
        catalog=conf["catalog"],
        schema=conf["schema"],
        source=conf["source"],
    )


# =============================================================================
# 1. DATE LOGIC (parity with SAS)
# =============================================================================

def compute_dates(today: date | None = None) -> dict:
    """
    Recreates SAS date logic from the original program.

    - SAS: end_dt = intnx('week.4', report_dt, 0) - 2;
      where 'week.4' is the Friday-based week interval.
    - We approximate that by finding the Friday of the week and subtracting 2 days.
    """
    if not today:
        today = date.today()

    start_dt_ini = date(2022, 6, 30)   # '30JUN2022'd
    week_end_dt_ini = date(2022, 7, 25)  # '25JUL2022'd

    weekday = today.weekday()  # Mon=0 ... Fri=4 ... Sun=6
    days_since_friday = (weekday - 4) % 7
    week_start_friday = today - timedelta(days=days_since_friday)

    end_dt = week_start_friday - timedelta(days=2)  # Wednesday

    if end_dt < week_end_dt_ini:
        start_dt = start_dt_ini
    else:
        start_dt = end_dt - timedelta(days=6)  # 7-day window

    week_start_dt = start_dt
    week_end_dt = end_dt
    pardt = start_dt - timedelta(days=7)
    week_end_dt_p1 = end_dt + timedelta(days=1)

    label = today.strftime("%Y%m%d")  # yyyymmdd

    return {
        "report_dt": today,
        "start_dt": start_dt,
        "end_dt": end_dt,
        "week_start_dt": week_start_dt,
        "week_end_dt": week_end_dt,
        "week_end_dt_p1": week_end_dt_p1,
        "pardt": pardt,
        "label": label,
        "ymd": label,
    }


# =============================================================================
# 2. SQL HELPERS (Trino dialect; CamelCase JSON paths)
# =============================================================================

# In Trino:
# - eventattributes: MAP(VARCHAR, VARCHAR)
# - element_at(eventattributes, 'key') → VARCHAR
# - json_parse(...) → JSON
# - json_extract_scalar(json, '$.path') → VARCHAR

ESS_PROCESS_TS = (
    "try(CAST(from_iso8601_timestamp("
    "element_at(eventattributes, 'ess_process_timestamp')) AS timestamp))"
)

ESS_SRC_TS = (
    "try(CAST(from_iso8601_timestamp("
    "element_at(eventattributes, 'ess_src_event_timestamp')) AS timestamp))"
)

SRC_HDR = "try(json_parse(element_at(eventattributes, 'SourceEventHeader')))"
EVT_PAYLOAD = "try(json_parse(element_at(eventattributes, 'eventPayload')))"
EVT_ID = f"try(json_extract_scalar({SRC_HDR}, '$.eventId'))"
EVT_TS = (
    f"try(CAST(from_iso8601_timestamp("
    f"json_extract_scalar({SRC_HDR}, '$.eventTimestamp')) AS timestamp))"
)


def q_xb80_cards(schema: str, week_start: date, week_end_p1: date, pardt: date) -> str:
    # partition_date is a yyyymmdd string in Hive/SAS; compare as string to avoid type mismatch
    pardt_str = pardt.strftime("%Y%m%d")
    return f"""
    SELECT
        event_activity_type,
        source_event_id,
        partition_date,
        {ESS_PROCESS_TS} AS ess_process_timestamp,
        {ESS_SRC_TS}     AS ess_src_event_timestamp,
        {EVT_ID}         AS eventId,
        {EVT_TS}         AS eventTimestamp,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.accountId'))       AS accountId,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.alertType'))       AS alertType,
        try(CAST(json_extract_scalar({EVT_PAYLOAD}, '$.thresholdAmount') AS DECIMAL(10,2))) AS thresholdAmount,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.customerID'))      AS customerID,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.accountCurrency')) AS accountCurrency,
        try(CAST(json_extract_scalar({EVT_PAYLOAD}, '$.creditLimit') AS DECIMAL(10,2)))     AS creditLimit,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.maskedAccount'))   AS maskedAccount,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.decisionId'))      AS decisionId,
        try(CAST(json_extract_scalar({EVT_PAYLOAD}, '$.alertAmount') AS DECIMAL(10,2)))     AS alertAmount
    FROM {schema}.xb80_credit_card_system_interface
    WHERE
        partition_date > '{pardt_str}'
        AND event_activity_type = 'Alert Decision Cards'
        AND try(json_extract_scalar({EVT_PAYLOAD}, '$.alertType')) = 'AVAIL_CREDIT_REMAINING'
        AND event_timestamp BETWEEN TIMESTAMP '{week_start.isoformat()} 00:00:00'
                               AND TIMESTAMP '{week_end_p1.isoformat()} 00:00:00'
    """


def q_cards_dec_pref(schema: str, week_end_p1: date) -> str:
    week_end_p1_str = f"{week_end_p1.isoformat()} 00:00:00"

    a = f"""
        SELECT
            {EVT_TS} AS event_timestamp_p,
            event_channel_type AS event_channel_type_p,
            event_activity_type AS event_activity_type_p,
            partition_date AS partition_date_p,
            {ESS_PROCESS_TS} AS ess_process_timestamp_p,
            {ESS_SRC_TS} AS ess_src_event_timestamp_p,
            {EVT_TS} AS eventTimestamp_p,
            try(json_extract_scalar({SRC_HDR}, '$.eventActivityName')) AS eventActivityName_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.preferenceType')) AS preferenceType_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.clientID')) AS clientID_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.isBusiness')) AS isBusiness_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.sendAlertEligible')) AS sendAlertEligible_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.active')) AS active_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.threshold')) AS threshold_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.custID')) AS custID_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.account')) AS account_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.maskAccountNo')) AS maskedAccountNo_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.externalAccount')) AS externalAccount_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.productType')) AS productType_p
        FROM {schema}.ffs0_client_alert_preferences_dep_initial_load
        WHERE event_activity_type IN ('Create Account Preference','Update Account Preference')
          AND try(json_extract_scalar({EVT_PAYLOAD}, '$.preferenceType')) = 'AVAIL_CREDIT_REMAINING'
          AND try(json_extract_scalar({EVT_PAYLOAD}, '$.productType'))   = 'CREDIT_CARD'
          AND partition_date = '20220324'
    """

    b = f"""
        SELECT
            {EVT_TS} AS event_timestamp_p,
            event_channel_type AS event_channel_type_p,
            event_activity_type AS event_activity_type_p,
            partition_date AS partition_date_p,
            {ESS_PROCESS_TS} AS ess_process_timestamp_p,
            {ESS_SRC_TS} AS ess_src_event_timestamp_p,
            {EVT_TS} AS eventTimestamp_p,
            try(json_extract_scalar({SRC_HDR}, '$.eventActivityName')) AS eventActivityName_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.preferenceType')) AS preferenceType_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.clientID')) AS clientID_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.isBusiness')) AS isBusiness_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.sendAlertEligible')) AS sendAlertEligible_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.active')) AS active_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.threshold')) AS threshold_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.custID')) AS custID_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.account')) AS account_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.maskAccountNo')) AS maskedAccountNo_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.externalAccount')) AS externalAccount_p,
            try(json_extract_scalar({EVT_PAYLOAD}, '$.productType')) AS productType_p
        FROM {schema}.ffs0_client_alert_preferences_dep
        WHERE event_timestamp > TIMESTAMP '{week_end_p1_str}'
          AND event_activity_type IN ('Create Account Preference','Update Account Preference')
          AND try(json_extract_scalar({EVT_PAYLOAD}, '$.preferenceType')) = 'AVAIL_CREDIT_REMAINING'
          AND try(json_extract_scalar({EVT_PAYLOAD}, '$.productType'))   = 'CREDIT_CARD'
    """

    # SAS used UNION (distinct), not UNION ALL
    return f"SELECT * FROM ({a}) UNION SELECT * FROM ({b})"


def q_fft0_inbox(schema: str, week_start: date) -> str:
    week_start_str = f"{week_start.isoformat()} 00:00:00"
    return f"""
    SELECT
        {ESS_PROCESS_TS} AS ess_process_timestamp_a,
        event_activity_type AS event_activity_type_a,
        source_event_id AS source_event_id_a,
        partition_date AS partition_date_a,
        event_timestamp AS event_timestamp_a,
        {EVT_ID} AS eventId_a,
        {EVT_TS} AS eventTimestamp_a,
        try(json_extract_scalar({SRC_HDR}, '$.eventActivityName')) AS eventActivityName_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.alertSent')) AS alertSent_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.sendInbox')) AS sendInbox_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.alertType')) AS alertType_a,
        try(CAST(json_extract_scalar({EVT_PAYLOAD}, '$.thresholdAmount') AS DECIMAL(12,2))) AS thresholdAmount_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.sendSMS')) AS sendSMS_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.sendPush')) AS sendPush_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.maskedAccount')) AS maskedAccount_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.reasonCode')) AS reasonCode_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.decisionId')) AS decisionId_a,
        try(CAST(json_extract_scalar({EVT_PAYLOAD}, '$.alertAmount') AS DECIMAL(12,2))) AS alertAmount_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.accountID')) AS accountId_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.account')) AS account_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.accountProduct')) AS accountProduct_a,
        try(json_extract_scalar({EVT_PAYLOAD}, '$.sendEmail')) AS sendEmail_a
    FROM {schema}.fft0_alert_inbox_dep
    WHERE event_activity_type = 'Alert Delivery Audit'
      AND try(json_extract_scalar({EVT_PAYLOAD}, '$.alertType')) = 'AVAIL_CREDIT_REMAINING'
      AND event_timestamp > TIMESTAMP '{week_start_str}'
    """


def read_trino_df(conn, sql: str) -> pd.DataFrame:
    logging.info("Running SQL:\n%s", sql)
    return pd.read_sql(sql, conn)


# =============================================================================
# 3. DATA PROCESSING (pandas parity with SAS)
# =============================================================================

def run_pipeline(conf: dict):
    # Dates / label
    dt = compute_dates()
    label = dt["label"]

    out_root = Path(conf["out_root"])
    log_root = Path(conf["log_root"])

    out_dir = out_root / label
    out_dir.mkdir(parents=True, exist_ok=True)
    log_root.mkdir(parents=True, exist_ok=True)

    log_file = log_root / f"CB600J_Alerts_Cards_{label}.log"
    setup_logging(log_file)

    logging.info("Environment=%s  Catalog=%s Schema=%s", conf["env"], conf["catalog"], conf["schema"])
    logging.info("Output dir: %s", out_dir.resolve())

    # -------------------------------------------------------------------------
    # 3.1 Pull data from Trino
    # -------------------------------------------------------------------------
    with get_trino_conn(conf) as conn:
        schema = f"{conf['catalog']}.{conf['schema']}"

        df_cards = read_trino_df(
            conn,
            q_xb80_cards(schema, dt["week_start_dt"], dt["week_end_dt_p1"], dt["pardt"])
        )
        df_pref = read_trino_df(conn, q_cards_dec_pref(schema, dt["week_end_dt_p1"]))
        df_inbox = read_trino_df(conn, q_fft0_inbox(schema, dt["week_start_dt"]))

    # Normalize all column names to lowercase for stable handling
    for df in (df_cards, df_pref, df_inbox):
        df.columns = df.columns.str.lower()

    logging.info("df_cards: %d rows, cols=%s", len(df_cards), df_cards.columns.tolist())
    logging.info("df_pref: %d rows, cols=%s", len(df_pref), df_pref.columns.tolist())
    logging.info("df_inbox: %d rows, cols=%s", len(df_inbox), df_inbox.columns.tolist())

    # Convert any *timestamp*-like columns to datetimes
    for df in (df_cards, df_pref, df_inbox):
        for col in df.columns:
            if "timestamp" in col:
                df[col] = pd.to_datetime(df[col], errors="coerce")

    # -------------------------------------------------------------------------
    # 3.2 Join xb80_cards with cards_dec_pref (SAS PROC SQL + DATA step)
    # -------------------------------------------------------------------------
    df_cards = df_cards.rename(columns={"accountid": "accountid_key", "customerid": "customerid_key"})
    df_pref = df_pref.rename(columns={"externalaccount_p": "externalaccount_p_key", "custid_p": "custid_p_key"})

    df_join = pd.merge(
        df_cards,
        df_pref,
        how="left",
        left_on=["accountid_key", "customerid_key"],
        right_on=["externalaccount_p_key", "custid_p_key"],
        suffixes=("", "_p2"),
    )

    # dec_tm_ge_pref_tm = 'Y' if eventTimestamp > eventTimestamp_p else 'N'
    df_join["dec_tm_ge_pref_tm"] = np.where(
        (df_join["eventtimestamp"].notna()) & (df_join["eventtimestamp_p"].notna()) &
        (df_join["eventtimestamp"] > df_join["eventtimestamp_p"]),
        "Y",
        "N",
    )

    # Sort and deduplicate by decisionid, mimicking SAS sort + NODUPKEY
    df_join = df_join.sort_values(
        by=[
            "decisionid",
            "eventtimestamp",
            "accountid_key",
            "customerid_key",
            "dec_tm_ge_pref_tm",
            "eventtimestamp_p",
            "externalaccount_p_key",
        ],
        ascending=[
            True,   # decisionid
            True,   # eventtimestamp
            True,   # accountid
            True,   # customerid
            False,  # dec_tm_ge_pref_tm (Y before N)
            False,  # eventtimestamp_p
            False,  # externalaccount_p_key
        ],
    )

    df_cards_dec_pref2 = df_join.drop_duplicates(subset=["decisionid"], keep="first")

    # -------------------------------------------------------------------------
    # 3.3 Prepare inbox (fft0_inbox2 in SAS)
    # -------------------------------------------------------------------------
    df_inbox = df_inbox.sort_values(
        by=["decisionid_a", "eventtimestamp_a"],
        ascending=[True, False],
    ).drop_duplicates(subset=["decisionid_a"], keep="first")

    # Join on decisionId
    df_all = pd.merge(
        df_cards_dec_pref2,
        df_inbox,
        how="left",
        left_on="decisionid",
        right_on="decisionid_a",
        suffixes=("", "_a2"),
    )

    # -------------------------------------------------------------------------
    # 3.4 cards_alert_final logic
    # -------------------------------------------------------------------------
    def to_bool_str(x):
        return str(x).lower()

    df_final = df_all.copy()
    # Drop business clients where decision time >= pref time
    df_final = df_final[~(
        (df_final["isbusiness_p"].map(to_bool_str) == "true") &
        (df_final["dec_tm_ge_pref_tm"] == "Y")
    )]

    for col in ["ess_src_event_timestamp", "ess_process_timestamp",
                "eventtimestamp", "eventtimestamp_a"]:
        if col in df_final.columns:
            df_final[col] = pd.to_datetime(df_final[col], errors="coerce")

    df_final["src_event_date"] = df_final["ess_src_event_timestamp"].dt.date
    df_final["process_date"] = df_final["ess_process_timestamp"].dt.date

    # SAS: decision_date = datepart(eventTimestamp)
    df_final["decision_date"] = df_final["eventtimestamp"].dt.date
    df_final["event_date"] = df_final["eventtimestamp"].dt.date

    df_final["alertamount"] = pd.to_numeric(df_final["alertamount"], errors="coerce")
    df_final["thresholdamount"] = pd.to_numeric(df_final["thresholdamount"], errors="coerce")
    df_final["threshold_limit_check"] = np.where(
        (df_final["alertamount"].notna()) &
        (df_final["thresholdamount"].notna()) &
        (df_final["alertamount"] < df_final["thresholdamount"]),
        "Y",
        "N",
    )

    df_final["Found_Missing"] = np.where(
        df_final["eventtimestamp_a"].isna() | df_final["eventtimestamp"].isna(),
        "Y",
        "N",
    )

    df_final["Time_Diff"] = (df_final["eventtimestamp_a"] - df_final["eventtimestamp"]).dt.total_seconds()

    df_final["SLA_Ind"] = np.where(
        (df_final["Time_Diff"].notna()) & (df_final["Time_Diff"] <= 1800),
        "Y",
        "N",
    )

    bins = [
        -float("inf"), 0, 1800, 3600, 7200, 10800, 14400, 18000, 21600,
        25200, 28800, 32400, 36000, 86400, 172800, 259200, float("inf"),
    ]
    labels = [
        "00 - Less than 0 seconds",
        "01 - Less than or equal to 30 minutes",
        "02 - Greater than 30 mins and less than or equal to 60 mins",
        "03 - Greater than 1 hour and less than or equal to 2 hours",
        "04 - Greater than 2 hours and less than or equal to 3 hours",
        "05 - Greater than 3 hours and less than or equal to 4 hours",
        "06 - Greater than 4 hours and less than or equal to 5 hours",
        "07 - Greater than 5 hours and less than or equal to 6 hours",
        "08 - Greater than 6 hours and less than or equal to 7 hours",
        "09 - Greater than 7 hours and less than or equal to 8 hours",
        "10 - Greater than 8 hours and less than or equal to 9 hours",
        "11 - Greater than 9 hours and less than or equal to 10 hours",
        "12 - Greater than 10 hours and less than or equal to 24 hours",
        "13 - Greater than 1 day and less than or equal to 2 days",
        "14 - Greater than 2 days and less than or equal to 3 days",
        "15 - Greater than 3 days",
    ]

    df_final["Alert_Time"] = pd.cut(df_final["Time_Diff"], bins=bins, labels=labels, right=True)
    df_final["Alert_Time"] = df_final["Alert_Time"].astype("category").cat.add_categories(
        "99 - Timestamp is missing"
    )
    df_final["Alert_Time"] = df_final["Alert_Time"].fillna("99 - Timestamp is missing")

    # -------------------------------------------------------------------------
    # 3.5 Accuracy / Timeliness / Completeness summaries
    # -------------------------------------------------------------------------
    dt_all = dt
    report_dt = dt_all["report_dt"]

    def snapdate(d: date):
        if pd.isna(d):
            return pd.NaT
        weekday = d.weekday()  # Wed=2
        days_to_wed_end = (2 - weekday) % 7
        return d + timedelta(days=days_to_wed_end)

    def sample_by_day(df, size=10, seed=42):
        out = []
        for _, g in df.groupby("decision_date"):
            if len(g) <= size:
                out.append(g.copy())
            else:
                out.append(g.sample(n=size, random_state=seed))
        return pd.concat(out, ignore_index=True) if out else df.head(0)

    sample_df = sample_by_day(df_final, size=10)

    acc = (
        sample_df.assign(
            ControlRisk="Accuracy",
            TestType="Sample",
            RDE="Alert010_Accuracy_Available_Credit",
            CommentCode=lambda d: np.where(d["threshold_limit_check"] == "Y", "COM16", "COM19"),
            Segment10=lambda d: pd.to_datetime(d["decision_date"]).dt.strftime("%Y%m%d"),
            DateCompleted=report_dt,
            SnapDate=lambda d: pd.to_datetime(d["decision_date"]).map(snapdate),
        )
        .groupby(
            ["ControlRisk", "TestType", "RDE", "CommentCode", "Segment10", "DateCompleted", "SnapDate"],
            dropna=False,
        )
        .agg(
            Volume=("decisionid", "count"),
            bal=("alertamount", "sum"),
            Amount=("thresholdamount", "sum"),
        )
        .reset_index()
    )

    tml = (
        df_final.assign(
            ControlRisk="Timeliness",
            TestType="Anomaly",
            RDE="Alert011_Timeliness_SLA",
            CommentCode=lambda d: np.where(d["SLA_Ind"] == "Y", "COM16", "COM19"),
            Segment10=lambda d: pd.to_datetime(d["decision_date"]).dt.strftime("%Y%m%d"),
            DateCompleted=report_dt,
            SnapDate=lambda d: pd.to_datetime(d["decision_date"]).map(snapdate),
        )
        .groupby(
            ["ControlRisk", "TestType", "RDE", "CommentCode", "Segment10", "DateCompleted", "SnapDate"],
            dropna=False,
        )
        .agg(
            Volume=("decisionid", "count"),
            bal=("alertamount", "sum"),
            Amount=("thresholdamount", "sum"),
        )
        .reset_index()
    )

    cpl = (
        df_final.assign(
            ControlRisk="Completeness",
            TestType="Reconciliation",
            RDE="Alert012_Completeness_All_Clients",
            CommentCode=lambda d: np.where(d["decisionid_a"].fillna("") != "", "COM16", "COM19"),
            Segment10=lambda d: pd.to_datetime(d["decision_date"]).dt.strftime("%Y%m"),
            DateCompleted=report_dt,
            SnapDate=lambda d: pd.to_datetime(d["decision_date"]).map(snapdate),
        )
        .groupby(
            ["ControlRisk", "TestType", "RDE", "CommentCode", "Segment10", "DateCompleted", "SnapDate"],
            dropna=False,
        )
        .agg(
            Volume=("decisionid", "count"),
            bal=("alertamount", "sum"),
            Amount=("thresholdamount", "sum"),
        )
        .reset_index()
    )

    alert_cards_ac_week = pd.concat([acc, tml, cpl], ignore_index=True)

    alert_cards_ac_week.insert(0, "RegulatoryName", "CB6")
    alert_cards_ac_week.insert(1, "LOB", "Credit Cards")
    alert_cards_ac_week.insert(2, "ReportName", "CB6 Alerts")
    alert_cards_ac_week.insert(5, "TestPeriod", "Portfolio")
    alert_cards_ac_week.insert(6, "ProductType", "Credit Cards")

    for col in [
        "SubDE",
        "Segment",
        "Segment2",
        "Segment3",
        "Segment4",
        "Segment5",
        "Segment6",
        "Segment7",
        "Segment8",
        "Segment9",
        "HoldoutFlag",
    ]:
        alert_cards_ac_week[col] = "."

    alert_cards_ac_week["HoldoutFlag"] = "N"
    alert_cards_ac_week["Comments"] = ""  # placeholder for SAS $cmt. format

    # -------------------------------------------------------------------------
    # 3.6 Detail tabs (Completeness_Fail, Timeliness_Fail, Accuracy_Fail)
    # -------------------------------------------------------------------------
    completeness_fail = (
        df_final[df_final["decisionid_a"].fillna("") == ""]
        .assign(
            event_month=lambda d: pd.to_datetime(d["decision_date"]).dt.strftime("%Y%m"),
            reporting_date=report_dt,
            event_week_ending=lambda d: pd.to_datetime(d["decision_date"]).map(snapdate),
            LOB="Credit Cards",
            Product="Credit Cards",
            account_number=lambda d: d["accountid_key"],
            available_credit=lambda d: d["alertamount"],
            event_date=lambda d: pd.to_datetime(d["decision_date"]).dt.date,
            custid_mask=lambda d: d["customerid_key"].fillna("").str[-3:].radd("******"),
        )[
            [
                "event_month",
                "reporting_date",
                "event_week_ending",
                "LOB",
                "Product",
                "account_number",
                "thresholdamount",
                "available_credit",
                "decisionid",
                "event_date",
                "custid_mask",
            ]
        ]
    )

    timeliness_fail = (
        df_final[df_final["SLA_Ind"] != "Y"]
        .assign(
            event_month=lambda d: pd.to_datetime(d["decision_date"]).dt.strftime("%Y%m"),
            reporting_date=report_dt,
            event_week_ending=lambda d: pd.to_datetime(d["decision_date"]).map(snapdate),
            LOB="Credit Cards",
            Product="Credit Cards",
            account_number=lambda d: d["accountid_key"],
            available_credit=lambda d: d["alertamount"],
            event_date=lambda d: pd.to_datetime(d["decision_date"]).dt.date,
            total_minutes=lambda d: (d["Time_Diff"] / 60.0).apply(
                lambda x: None if pd.isna(x) else math.ceil(x)
            ),
            custid_mask=lambda d: d["customerid_key"].fillna("").str[-3:].radd("******"),
        )[
            [
                "event_month",
                "reporting_date",
                "event_week_ending",
                "LOB",
                "Product",
                "account_number",
                "thresholdamount",
                "available_credit",
                "decisionid",
                "event_date",
                "eventtimestamp",
                "eventtimestamp_a",
                "total_minutes",
                "custid_mask",
            ]
        ]
    )

    accuracy_fail = (
        sample_df[sample_df["threshold_limit_check"] != "Y"]
        .assign(
            event_month=lambda d: pd.to_datetime(d["decision_date"]).dt.strftime("%Y%m"),
            reporting_date=report_dt,
            event_week_ending=lambda d: pd.to_datetime(d["decision_date"]).map(snapdate),
            LOB="Credit Cards",
            Product="Credit Cards",
            account_number=lambda d: d["accountid_key"],
            decision="AlertDecision",
            available_credit=lambda d: d["alertamount"],
            event_date=lambda d: pd.to_datetime(d["decision_date"]).dt.date,
            custid_mask=lambda d: d["customerid_key"].fillna("").str[-3:].radd("******"),
        )[
            [
                "event_month",
                "reporting_date",
                "event_week_ending",
                "LOB",
                "Product",
                "account_number",
                "decision",
                "thresholdamount",
                "available_credit",
                "decisionid",
                "event_date",
                "custid_mask",
            ]
        ]
    )

    # -------------------------------------------------------------------------
    # 3.7 OUTPUTS (parquet + Excel)
    # -------------------------------------------------------------------------
    df_final_path = out_dir / "cards_alert_final.parquet"
    df_final.to_parquet(df_final_path, index=False)
    logging.info("Wrote %s", df_final_path)

    ac_week_path = out_dir / "Alert_Cards_AC_week.parquet"
    alert_cards_ac_week.to_parquet(ac_week_path, index=False)
    logging.info("Wrote %s", ac_week_path)

    # Also export key tables as Excel to mirror SAS PROC EXPORT
    df_final_xlsx = out_dir / "cards_alert_final.xlsx"
    df_final.to_excel(df_final_xlsx, sheet_name="cards_alert_final", index=False)
    logging.info("Wrote %s", df_final_xlsx)

    ac_week_xlsx = out_dir / "Alert_Cards_AC_week.xlsx"
    alert_cards_ac_week.to_excel(ac_week_xlsx, sheet_name="Alert_Cards_AC_week", index=False)
    logging.info("Wrote %s", ac_week_xlsx)

    xlsx1 = out_dir / "Alert_Cards_Completeness_Detail.xlsx"
    completeness_fail.to_excel(xlsx1, sheet_name="Alert_Cards_Completeness_Detail", index=False)
    logging.info("Wrote %s", xlsx1)

    xlsx2 = out_dir / "Alert_Cards_Timeliness_Detail.xlsx"
    timeliness_fail.to_excel(xlsx2, sheet_name="Alert_Cards_Timeliness_Detail", index=False)
    logging.info("Wrote %s", xlsx2)

    xlsx3 = out_dir / "Alert_Cards_Accuracy_Detail.xlsx"
    accuracy_fail.to_excel(xlsx3, sheet_name="Alert_Cards_Accuracy_Detail", index=False)
    logging.info("Wrote %s", xlsx3)

    logging.info(
        "Final counts: rows=%d, distinct decisionId=%d",
        len(df_final),
        df_final["decisionid"].nunique(),
    )

    # -------------------------------------------------------------------------
    # 3.8 Historical append logic (like ac.alert_cards_ac)
    # -------------------------------------------------------------------------
    logging.info("Updating historical autocomplete file...")

    ac_lib_path = Path(conf["out_root"]).parent
    ac_lib_path.mkdir(parents=True, exist_ok=True)
    historical_ac_file = ac_lib_path / "alert_cards_ac.parquet"

    df_new_week = alert_cards_ac_week.copy()
    df_new_week["SnapDate"] = pd.to_datetime(df_new_week["SnapDate"]).dt.date

    if historical_ac_file.exists():
        logging.info("Reading existing history file: %s", historical_ac_file)
        df_history = pd.read_parquet(historical_ac_file)
        df_history["SnapDate"] = pd.to_datetime(df_history["SnapDate"]).dt.date

        new_snapdates = df_new_week["SnapDate"].unique()
        df_history_filtered = df_history[~df_history["SnapDate"].isin(new_snapdates)]
        df_combined_history = pd.concat([df_history_filtered, df_new_week], ignore_index=True)
        logging.info("Appended new data, replacing %d existing SnapDate(s).", len(new_snapdates))
    else:
        logging.warning("History file not found. Creating new one.")
        df_combined_history = df_new_week

    df_combined_history = df_combined_history.sort_values(by="SnapDate")
    df_combined_history.to_parquet(historical_ac_file, index=False)
    logging.info("Wrote updated historical file: %s (Total rows: %d)", historical_ac_file, len(df_combined_history))


# =============================================================================
# 4. MAIN
# =============================================================================

if __name__ == "__main__":
    cfg = load_config(Path("config.ini"))
    run_pipeline(cfg)
